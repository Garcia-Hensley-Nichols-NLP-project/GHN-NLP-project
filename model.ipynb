{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prepare import basic_clean, tokenize, stem, lemmatize, remove_stopwords, prep_article_data, words\n",
    "from acquire import scrape_github_data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Decision Tree and Random Forest ;D\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')\n",
    "\n",
    "df = words(df)\n",
    "\n",
    "#df.info()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 1) (24, 1) (20, 1)\n",
      "(56, 1) (24, 1) (20, 1)\n",
      "(56, 1) (24, 1) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "# Make copies of df with prepared columns and target\n",
    "clean_df = df.copy()[['language', 'clean']]\n",
    "stem_df = df.copy()[['language', 'stemmed']]\n",
    "lem_df = df.copy()[['language', 'lemmatized']]\n",
    "\n",
    "\n",
    "# Get splits for each of the above dfs and isolate target\n",
    "X_clean = clean_df[['clean']]\n",
    "y_clean = clean_df.language\n",
    "\n",
    "X_clean_train, X_clean_test, y_clean_train, y_clean_test = train_test_split(X_clean, y_clean, test_size=.2, random_state=302)\n",
    "X_clean_train, X_clean_validate, y_clean_train, y_clean_validate  = train_test_split(X_clean_train, y_clean_train, test_size=.3, random_state=302)\n",
    "\n",
    "print(X_clean_train.shape, X_clean_validate.shape, X_clean_test.shape)\n",
    "\n",
    "X_stem = stem_df[['stemmed']]\n",
    "y_stem = stem_df.language\n",
    "\n",
    "X_stem_train, X_stem_test, y_stem_train, y_stem_test = train_test_split(X_stem, y_stem, test_size=.2, random_state=302)\n",
    "X_stem_train, X_stem_validate, y_stem_train, y_stem_validate  = train_test_split(X_stem_train, y_stem_train, test_size=.3, random_state=302)\n",
    "\n",
    "print(X_stem_train.shape, X_stem_validate.shape, X_stem_test.shape)\n",
    "\n",
    "X_lem = lem_df[['lemmatized']]\n",
    "y_lem = lem_df.language\n",
    "\n",
    "X_lem_train, X_lem_test, y_lem_train, y_lem_test = train_test_split(X_lem, y_lem, test_size=.2, random_state=302)\n",
    "X_lem_train, X_lem_validate, y_lem_train, y_lem_validate  = train_test_split(X_lem_train, y_lem_train, test_size=.3, random_state=302)\n",
    "\n",
    "print(X_lem_train.shape, X_lem_validate.shape, X_lem_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make vectorizer objects for bags of words (clean_df)\n",
    "# cv_clean = CountVectorizer()\n",
    "# tfidf_clean = TfidfVectorizer()\n",
    "\n",
    "# #Bags of words\n",
    "# cv_clean_bow = cv_clean.fit_transform(X_clean_train.clean)\n",
    "# tf_clean_bow = tfidf_clean.fit_transform(X_clean_train.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make and fit decision tree object for cv_clean_bow\n",
    "# cv_tree1 = DecisionTreeClassifier(max_depth=5)\n",
    "# cv_tree1.fit(cv_clean_bow, y_clean_train)\n",
    "\n",
    "# #Output tree score\n",
    "# print(f'CV tree 1 score: {cv_tree1.score(cv_clean_bow, y_clean_train)}')\n",
    "\n",
    "# #Make and fit decision tree object for tf_clean_bow\n",
    "# tf_tree1 = DecisionTreeClassifier(max_depth=5)\n",
    "# tf_tree1.fit(tf_clean_bow, y_clean_train)\n",
    "\n",
    "# #Output tree score\n",
    "# print(f'TF IDF tree 1 score: {tf_tree1.score(tf_clean_bow, y_clean_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"Stemmed\" models\n",
    "# cv_stem = CountVectorizer()\n",
    "# tfidf_stem = TfidfVectorizer()\n",
    "\n",
    "# # Bags\n",
    "# cv_stem_bow = cv_stem.fit_transform(X_stem_train.stemmed)\n",
    "# tf_stem_bow = tfidf_stem.fit_transform(X_stem_train.stemmed)\n",
    "\n",
    "# # Make and fit decision tree object for cv_stem_bow\n",
    "# cv_tree2 = DecisionTreeClassifier(max_depth=5)\n",
    "# cv_tree2.fit(cv_stem_bow, y_stem_train)\n",
    "\n",
    "# #Output tree score\n",
    "# print(f'CV tree 2 score: {cv_tree2.score(cv_stem_bow, y_stem_train)}')\n",
    "\n",
    "# #Make and fit decision tree object for tf_stem_bow\n",
    "# tf_tree2 = DecisionTreeClassifier(max_depth=5)\n",
    "# tf_tree2.fit(tf_stem_bow, y_stem_train)\n",
    "\n",
    "# #Output tree score\n",
    "# print(f'TF IDF tree 2 score: {tf_tree2.score(tf_stem_bow, y_stem_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \"Lemmatized\" models\n",
    "# cv_lem = CountVectorizer()\n",
    "# tfidf_lem = TfidfVectorizer()\n",
    "\n",
    "# cv_lem_bow = cv_lem.fit_transform(X_lem_train.lemmatized)\n",
    "# tf_lem_bow = tfidf_lem.fit_transform(X_lem_train.lemmatized)\n",
    "\n",
    "# # Make and fit decision tree object for cv_lem_bow\n",
    "# cv_tree3 = DecisionTreeClassifier(max_depth=5)\n",
    "# cv_tree3.fit(cv_lem_bow, y_lem_train)\n",
    "\n",
    "# #Make and fit decision tree object for tf_lem_bow\n",
    "# tf_tree3 = DecisionTreeClassifier(max_depth=5)\n",
    "# tf_tree3.fit(tf_lem_bow, y_lem_train)\n",
    "\n",
    "# # Output tree scores\n",
    "# print(f'CV tree score: {cv_tree3.score(cv_lem_bow, y_lem_train)}') \n",
    "# print(f'TFIDF tree score: {tf_tree3.score(tf_lem_bow, y_lem_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec_tree_training_scores= {\n",
    "#     'CV_clean': cv_tree1.score(cv_clean_bow, y_clean_train),\n",
    "#     'CV_stem': cv_tree2.score(cv_stem_bow, y_stem_train),\n",
    "#     'CV_lem': cv_tree3.score(cv_lem_bow, y_lem_train),\n",
    "#     'TFIDF_clean': tf_tree1.score(tf_clean_bow, y_clean_train),\n",
    "#     'TFIDF_stem': tf_tree2.score(tf_stem_bow, y_stem_train),\n",
    "#     'TFIDF_lem': tf_tree3.score(tf_lem_bow, y_lem_train)\n",
    "# }\n",
    "# dec_tree_training_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classi_bow(X_lem_train,y_lem_train,X_stem_train,y_stem_train,X_clean_train,y_clean_train):\n",
    "\n",
    "    # Make vectorizer objects for bags of words (clean_df)\n",
    "    cv_clean = CountVectorizer()\n",
    "    tfidf_clean = TfidfVectorizer()\n",
    "\n",
    "    #Bags of words\n",
    "    cv_clean_bow = cv_clean.fit_transform(X_clean_train[['clean']].clean)\n",
    "    tf_clean_bow = tfidf_clean.fit_transform(X_clean_train[['clean']].clean)\n",
    "    \n",
    "    # Make and fit decision tree object for cv_clean_bow\n",
    "    cv_tree1 = DecisionTreeClassifier(max_depth=5)\n",
    "    cv_tree1.fit(cv_clean_bow, y_clean_train)\n",
    "\n",
    "    #Make and fit decision tree object for tf_clean_bow\n",
    "    tf_tree1 = DecisionTreeClassifier(max_depth=5)\n",
    "    tf_tree1.fit(tf_clean_bow, y_clean_train)\n",
    "\n",
    "\n",
    "    # \"Stemmed\" models\n",
    "    cv_stem = CountVectorizer()\n",
    "    tfidf_stem = TfidfVectorizer()\n",
    "\n",
    "    # Bags\n",
    "    cv_stem_bow = cv_stem.fit_transform(X_stem_train[['stemmed']].stemmed)\n",
    "    tf_stem_bow = tfidf_stem.fit_transform(X_stem_train[['stemmed']].stemmed)\n",
    "\n",
    "    # Make and fit decision tree object for cv_stem_bow\n",
    "    cv_tree2 = DecisionTreeClassifier(max_depth=5)\n",
    "    cv_tree2.fit(cv_stem_bow, y_stem_train)\n",
    "\n",
    "    # Make and fit decision tree object for tf_stem_bow\n",
    "    tf_tree2 = DecisionTreeClassifier(max_depth=5)\n",
    "    tf_tree2.fit(tf_stem_bow, y_stem_train)\n",
    "\n",
    "\n",
    "    # \"Lemmatized\" models\n",
    "    cv_lem = CountVectorizer()\n",
    "    tfidf_lem = TfidfVectorizer()\n",
    "\n",
    "    # Bags\n",
    "    cv_lem_bow = cv_lem.fit_transform(X_lem_train[['lemmatized']].lemmatized)\n",
    "    tf_lem_bow = tfidf_lem.fit_transform(X_lem_train[['lemmatized']].lemmatized)\n",
    "\n",
    "    # Make and fit decision tree object for cv_lem_bow\n",
    "    cv_tree3 = DecisionTreeClassifier(max_depth=5)\n",
    "    cv_tree3.fit(cv_lem_bow, y_lem_train)\n",
    "\n",
    "    #Make and fit decision tree object for tf_lem_bow\n",
    "    tf_tree3 = DecisionTreeClassifier(max_depth=5)\n",
    "    tf_tree3.fit(tf_lem_bow, y_lem_train)\n",
    "\n",
    "\n",
    "    dec_tree_training_scores=pd.Series({\n",
    "        'CV_clean': cv_tree1.score(cv_clean_bow, y_clean_train),\n",
    "        'CV_stem': cv_tree2.score(cv_stem_bow, y_stem_train),\n",
    "        'CV_lem': cv_tree3.score(cv_lem_bow, y_lem_train),\n",
    "        'TFIDF_clean': tf_tree1.score(tf_clean_bow, y_clean_train),\n",
    "        'TFIDF_stem': tf_tree2.score(tf_stem_bow, y_stem_train),\n",
    "        'TFIDF_lem': tf_tree3.score(tf_lem_bow, y_lem_train)\n",
    "    })\n",
    "\n",
    "    return dec_tree_training_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree_training_scores=classi_bow(X_lem_train,y_lem_train,X_stem_train,y_stem_train,X_clean_train,y_clean_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_clean       0.857143\n",
       "CV_stem        0.857143\n",
       "CV_lem         0.857143\n",
       "TFIDF_clean    0.875000\n",
       "TFIDF_stem     0.892857\n",
       "TFIDF_lem      0.928571\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tree_training_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways: Both vectorizers scoring the same on clean, stemmed, and lemmatized versions of README text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform validate split with vectorizer\n",
    "tf_clean_bow_val = tfidf_clean.transform(X_clean_validate.clean)\n",
    "\n",
    "#Get tf_tree1 score on validate\n",
    "tf_tree1.score(tf_clean_bow_val, y_clean_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerable drop off. Probably overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform validate split with vectorizer\n",
    "tf_stem_bow_val = tfidf_stem.transform(X_stem_validate.stemmed)\n",
    "\n",
    "#Get tf_tree1 score on validate\n",
    "tf_tree2.score(tf_stem_bow_val, y_stem_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even worse dropoff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform validate split with vectorizer\n",
    "tf_lem_bow_val = tfidf_lem.transform(X_lem_validate.lemmatized)\n",
    "\n",
    "#Get tf_tree1 score on validate\n",
    "tf_tree3.score(tf_lem_bow_val, y_lem_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized and stemmed preparations scored the same: 54.2%  Cleaned preparation scored 62.5% and will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_clean_bow_test = tfidf_clean.transform(X_clean_test.clean)\n",
    "\n",
    "tf_tree1.score(tf_clean_bow_test, y_clean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: Decision Tree model with a max depth of 5 and TFIDF Vectorizer, using \"clean\" version of README text, performs at 35% accuracy on unseen data.\n",
    "\n",
    "# Conclusions: Different types of models and adjustments to hyperparameters may yield better results."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
